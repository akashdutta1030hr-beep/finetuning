{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/unsloth/Qwen3-0.6B-Base\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/unsloth/Qwen3-0.6B-Base)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "XH6TFx7KwTHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"unsloth/Qwen3-0.6B-Base\")"
      ],
      "metadata": {
        "id": "5xH-8PnRwTHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-0.6B-Base\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen3-0.6B-Base\")"
      ],
      "metadata": {
        "id": "rHDRrLvGwTH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-0.6B-Base\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen3-0.6B-Base\", device_map = \"auto\")\n",
        "# Total parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "p_MkcZW9wWgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"vicgalle/alpaca-gpt4\")\n",
        "\n",
        "ds = ds[\"train\"]\n",
        "print(ds)\n",
        "dataset = ds.shuffle(seed = 42)\n",
        "print(dataset)\n",
        "\n",
        "train_dataset = dataset.select(range(2000))\n",
        "test_dataset = dataset.select(range(2000,2100))\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "DEDj5FdrwXPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 512\n",
        "\n",
        "def preprocess(example):\n",
        "    # Combine instruction + optional input + output\n",
        "    prompt = f\"<system> You are a helpful assistant. </system>\\n\" \\\n",
        "             f\"<user>{example['instruction']}\\n{example.get('input','')}</user>\\n\" \\\n",
        "             f\"<assistant>{example['output']}</assistant>\"\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Labels for causal LM\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = train_dataset.map(preprocess, batched=False)\n",
        "test_dataset = test_dataset.map(preprocess, batched=False)\n",
        "\n",
        "print(train_dataset[0])\n",
        "print(test_dataset[0])"
      ],
      "metadata": {
        "id": "oj6Me-jCwbKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PrefixTuningConfig, get_peft_model, TaskType\n",
        "\n",
        "prefix_config = PrefixTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    num_virtual_tokens=30,   # prefix length\n",
        ")\n",
        "\n",
        "# Apply prompt tuning to model\n",
        "model = get_peft_model(model, prefix_config)\n",
        "\n",
        "# Check trainable parameters\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "kqFFYYm_wd7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen3_prefix_tuning\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,  # effective batch = 8\n",
        "    learning_rate=2e-4,             # higher LR works well for prompt tuning\n",
        "    fp16=True,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "8iIsxFiLwqOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./qwen3_prifix_tuning\")"
      ],
      "metadata": {
        "id": "CGHw8_MJxJ_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}