{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWaDkTfdH2OI",
    "outputId": "6b0f2491-bd0d-4697-f46b-87e656d3d1a1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SaItLG6AGvoX",
    "outputId": "1f6380b6-760f-4f3f-bc8d-4a62a7273da9"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWbNogfMGvob"
   },
   "source": [
    "## Local Inference on GPU\n",
    "Model page: https://huggingface.co/unsloth/Qwen3-0.6B-Base\n",
    "\n",
    "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/unsloth/Qwen3-0.6B-Base)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446,
     "referenced_widgets": [
      "e0545ac18ea84723a7ab22c3558b7af1",
      "e4f622f2634d459c96e01d35d7b2fa57",
      "402e352137f9422db439926337f8dba4",
      "c1f963e5b3074e1ea795bcda6992e5b6",
      "3f3b1576124c4bff88940cd6e6c5f7b9",
      "f9209f6a94954fa79d9760631c88b577",
      "6be3e9c4ce684cd4b8daf73247b6f945",
      "44cea647a3d54201b3f1059d459f084a",
      "985653c76ce24a489b7f0a531f2e350a",
      "7c22b1eb185c4923a510e780a4f0b5eb",
      "1a5c7b84149d4703b5983849e6f3ead0",
      "f45c29bbc8ab446cb78d65ad9ee00a31",
      "fe27a9f74af649858ec9b99cdcea582d",
      "95e37782837347dcb4905d9016253f8b",
      "354ef29b57aa4d879438b65a7dda9ebc",
      "5b75bca0a4bd4c908cb0dbc202b8ffe3",
      "a5996277e7db44a5b4b7c0e75860086e",
      "1f8a07fb72b94edfac55a9f39d1decb3",
      "963b4e680dab4345a865b5d59fdd1a5b",
      "3252fd7821094a48b37af16d4d9e4a5b",
      "e130554d83324c3dbac3ab66c33cd5f4",
      "165566ec0c2144f4826c3f0297080cf3",
      "c7675e05b52a42a69ac6168c853519fc",
      "39893244640c4d80a74eeb5288329527",
      "5dd25ff823614b69b918cf4fda040f63",
      "baa0c282d7b640ecbc0e877fb9421b83",
      "48094420344e47028c42eb02d9ffab37",
      "7e6daa984add4b3e8e8135b6bc517022",
      "b7fd695122244c37a150939b404b8199",
      "4b75e8af71b2431a925f5f766a31aa30",
      "78e003b668154e0fa744c1227719b456",
      "05224e9cdcbd43a4ade3f9aed85a4195",
      "ebb0061060f342c5b2a26321a59c7d0d",
      "432a6c7c89404283a4911b633ee4022d",
      "48be82898b854bfb98cabccdce70c335",
      "add28fe54c4249d5a62d4e25186d36a7",
      "5e8b30bd468441aa895e20beb35b8569",
      "c527eb0ae94b40859d6dce0226537def",
      "5f804952ef714e0187923ef4fa4061cf",
      "99e2c12277004334b6aef82044ea3785",
      "49eeb14d708c4f3db47ab53b8cf11568",
      "5dbfe605a043402e8fde8836e910aa7b",
      "8d54df3fd8e84a86b3b98c55a5003396",
      "77eb6415e1a44c3aa8f0ce38cfdacb65",
      "78e0ef56f52a44888b13d21522734b05",
      "83bf37065eeb48368797ebae6a8daeff",
      "faf0a8e5764449d785c61dabbf4b2741",
      "4faaeb4d631a405c90543d428b1ebcae",
      "fe5779988ad7404b93f02d7a7021bf95",
      "6ffe26d8ab644ee99cd3ca2d33d9028a",
      "49b1253db9f34322997000df2c6dbb33",
      "b32c7bfabcad4b198dc8fcc6223a4429",
      "597512698a624136aa379b8da247354a",
      "ef6e4c4a2afe45b0ba3fa9acedb3c803",
      "acd1098fcfbe4f40b0c33370f68c995d",
      "c83b0e6387b24a649c0914a924dd42ff",
      "62a57bb52e284c3ba3714b2badbe2cd8",
      "15a41204c2d64f2885348533d7ee8d7b",
      "713c348f94cb457a81741c428f117fe5",
      "c5f4b8676a894f64998bc60672935358",
      "43acc54324984f27aa0786387ed0c792",
      "fc15b044d825492ca509d40b1ada0e85",
      "358de0df109b4abe8a665c36b817d7a6",
      "53fb4f1f0f444a0a9597c37b75f55f41",
      "084c7d9a76e14820b84d58a0a5addf78",
      "75d390ac27fe406c99e2374180b8d9c5",
      "ab3f8ca2998343d193bd2707d996b6c8",
      "62c8f7ed5b204461b99ba83f6a5905d3",
      "bf864e82085c4f2daa91a07495675ea4",
      "dae6c875731140a2858e0e83bd6606ab",
      "cb276054d62a46c28a0aa80138dee69e",
      "e59e343490c248e3ab4e6bf56aa913c9",
      "75cce186d64c44a1bf4c93b5c0d5ba64",
      "1ba430e1854a4abf9cd1ba43d3e33cf3",
      "784a1f94ee0b47dea24f2dc25241cf43",
      "729f45d4b5f24d25bf81284f484e7257",
      "7a88867c89cb4bc6ba950c5a1b60106f",
      "02cbbf7ec705402db6f179cc12823645",
      "debdbca4a729473e91a264cc6b4c68ff",
      "3e4507f930c648a8b3c83e00fd7e49a2",
      "77e2eb8ecd8f4cd690ce01b4d605f382",
      "aa7cb40e01d84e2fbe94b17332ace9cf",
      "49f071b80fb94ed680b9ba133e9ad352",
      "afd9c82d8d9048cab35052e8f4ab9a76",
      "df59f16b4dc047399b99c4b588066214",
      "989916b727604e0e9f50a14d534851a7",
      "54b606a8573f42889ac3dcf7d45b4ece",
      "39c9ee414336443e8b1845bd67c27a3c",
      "fef1289306d84913b4a590d77f51ce52",
      "70ecae25b27e4e988033950c23af172f",
      "31bfac2f98a149db99f51dd62dfa67cf",
      "3957252ad1c842ff8aef73d41d0dc0c1",
      "378896974ba44d08a9638a38943feb25",
      "582fd02f489c415e8fd53300b1797379",
      "edd694ede5034339b3ceb685fc3f711e",
      "bbe56247af8b4a428703489b02f90e1c",
      "7a65be39969f451b9d6ddeca221c5d59",
      "cc0324c19852488387f061424c2057b3",
      "1d7760da44664bd68b257c8d45fd713a"
     ]
    },
    "id": "yOjhUKS2Gvoe",
    "outputId": "e97116dc-0ad7-4399-fb17-e47188c365fe"
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"unsloth/Qwen3-0.6B-Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDZBSr1FGvog"
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "import bitsandbytes as bnb # Explicitly import bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-0.6B-Base\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/Qwen3-0.6B-Base\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "26dba490e5c24084a95aaa989e945c26",
      "44aeea108433448e953283e0a9069cd7",
      "ae80e57faab1432c80cb4d89fd37547c",
      "e5df24dd74a14f2b849399ba30b7ef53",
      "38666e707ef344c18d901af4dd7649ff",
      "80a1a5c06a4d42a1a69032098e401655",
      "77cfca8166da49a8889c612f929d31cd",
      "c69c43e3808340b290b5864f96177d7f",
      "71af4034c7224058b0936d21719feaf5",
      "faacb83a825e40b98873b2b2032e3032",
      "4717220ab1c34e7b80abd9c6e1ee0dda",
      "d94700a2c80946e4a200a0d18eb057bd",
      "b62df1f5bbf148d29419b4c62f0d939f",
      "fa82012d9af94df3a9ec05b12262815c",
      "a96be1f775f44359b65011cd11467ddf",
      "3d5678f2aacb4fdbbe8b587ee2f7fb07",
      "8c1544fdf9994a958f8495f1fc5a0c0a",
      "7a625fb036ed4cd9b7c78fccd14290b9",
      "ac800ba4ce63470c9cdf28b32484426e",
      "1928dc16b6ce4d5094018bf1edd3b705",
      "bf5497a9a96b48eea3e284d05b40e173",
      "4158202241144116ac95ee9e89fa3f65",
      "d413ee9827f4442285c322424c0d6d4e",
      "b194e29d0abb451e97026e52008dfb97",
      "91318ef1bf2b41638b60eba70ae7ebe2",
      "7f19421d55a542ab896ca359fdbe1884",
      "b675e264a5eb4c9084920df796a40024",
      "5670deed30ce4356b0b228735c7dc0ec",
      "a66dc87e7d5d442480b9b7de033fec1a",
      "0cd5c025121c4248be7a9b23dfcbeca3",
      "d8e86c7ef02b42d297054ec789a26f18",
      "4a6ea057d568424eb70411bb6c9c6684",
      "178dc8d136af4ea28f4ea691a581a744"
     ]
    },
    "id": "kEk6kzrtGx39",
    "outputId": "4f04efe7-7615-47e8-bca9-fc14724c525c"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"vicgalle/alpaca-gpt4\")\n",
    "\n",
    "ds = ds[\"train\"]\n",
    "print(ds)\n",
    "dataset = ds.shuffle(seed = 42)\n",
    "print(dataset)\n",
    "\n",
    "train_dataset = dataset.select(range(2000))\n",
    "test_dataset = dataset.select(range(2000,2100))\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "a57fc5cc65a941288f760c3425b219f3",
      "e104f6da0f6d4be38be82c34f1271dfd",
      "33a34c80eb1e497bbf14dccbf0a0d1e2",
      "0db780be653041109f16d14ffa7d0fd7",
      "3bd32f66cdd848129d2f696f5417b508",
      "76b9d25b67c84a65b877d96b54f61c42",
      "7a7f7c9a37ee4a11a79998669fa7677e",
      "30df71c8640f43d0bc7e44ee4c20dc3c",
      "457b472d11584e19bd306d29f3b671a8",
      "0be202b68b884f49b735afe6aaa9f711",
      "212a8b55037b43049b3b9a5f08ab37ee",
      "d7128c05d529461686803135999801c0",
      "1279e5c5fb5a4f5883bb09b95f34ea54",
      "f4f9713f42354d6db9f4f7342507390c",
      "f68e81e0055643dca5293a73e58df36f",
      "76dd83c2ef224c988bfd751a56c2ed29",
      "e2847f44113e4ed3abead679975b56b2",
      "0f6155337fe14ce7a8c75fb978aa9d80",
      "56c6cf2091d1408daab2b3231a738c4a",
      "416d428638bc44cf82e21696ffe366f1",
      "03ef64be9f2144dfaefe93db8f2fd6a4",
      "39266afa979b49c0972ee41c2743b0f6"
     ]
    },
    "id": "IUENFBBeG0fZ",
    "outputId": "bf056bcd-683e-4d19-e945-b488a777a83b"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "def preprocess(example):\n",
    "    # Combine instruction + optional input + output\n",
    "    prompt = f\"<system> You are a helpful assistant. </system>\\n\" \\\n",
    "             f\"<user>{example['instruction']}\\n{example.get('input','')}</user>\\n\" \\\n",
    "             f\"<assistant>{example['output']}</assistant>\"\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Labels for causal LM\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess, batched=False)\n",
    "test_dataset = test_dataset.map(preprocess, batched=False)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDT2Jqq5G6Ii",
    "outputId": "0faab11e-b12a-428b-d4cc-9944f76e989c"
   },
   "outputs": [],
   "source": [
    "!pip install peft\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "id": "7ONy694pHKUr",
    "outputId": "6e396fc9-35cf-4c43-c06e-1547a45bfbd9"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen3_qlora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # effective batch = 16\n",
    "    learning_rate=5e-5,             # higher LR works well for prompt tuning\n",
    "    fp16=True,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1j_Zc4C6Hu2Z"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./qwen3_QLoRA_tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXGMgR1RHs8N",
    "outputId": "7a646e33-fc83-4cd2-ae71-e62ed7cf2984"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "prompt = \"<system> You are a helpful assistant. </system>\\n<user>Explain gradient descent simply.</user>\\n<assistant>\"\n",
    "output = pipe(prompt)\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
