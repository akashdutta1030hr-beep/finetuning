{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "1zTBp-bc9hxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/unsloth/Qwen3-0.6B-Base\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/unsloth/Qwen3-0.6B-Base)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "OQwsI5ql39mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"unsloth/Qwen3-0.6B-Base\")"
      ],
      "metadata": {
        "id": "3BjVu_ht9gs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-0.6B-Base\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen3-0.6B-Base\", device_map=\"auto\")\n",
        "# Total parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "AajqODZb39m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"vicgalle/alpaca-gpt4\")\n",
        "\n",
        "ds = ds[\"train\"]\n",
        "print(ds)\n",
        "dataset = ds.shuffle(seed = 42)\n",
        "print(dataset)\n",
        "\n",
        "train_dataset = dataset.select(range(2000))\n",
        "test_dataset = dataset.select(range(2000,2100))\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "bzpMW59hIeMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 512\n",
        "\n",
        "def preprocess(example):\n",
        "    # Combine instruction + optional input + output\n",
        "    prompt = f\"<system> You are a helpful assistant. </system>\\n\" \\\n",
        "             f\"<user>{example['instruction']}\\n{example.get('input','')}</user>\\n\" \\\n",
        "             f\"<assistant>{example['output']}</assistant>\"\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Labels for causal LM\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = train_dataset.map(preprocess, batched=False)\n",
        "test_dataset = test_dataset.map(preprocess, batched=False)\n",
        "\n",
        "print(train_dataset[0])\n",
        "print(test_dataset[0])"
      ],
      "metadata": {
        "id": "HKOWhpyXQ1Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PromptTuningConfig, get_peft_model, TaskType\n",
        "\n",
        "prompt_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    num_virtual_tokens=50,  # number of soft prompt tokens\n",
        "    token_dim=1024,         # usually equal to hidden size of model\n",
        "    prompt_tuning_init=\"<system> You are a helpful assistant. </system>\",\n",
        ")\n",
        "\n",
        "# Apply prompt tuning to model\n",
        "model = get_peft_model(model, prompt_config)\n",
        "\n",
        "# Check trainable parameters\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "Bc0IUe-IKaRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen3_prompt_tuning\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,  # effective batch = 8\n",
        "    learning_rate=1e-3,             # higher LR works well for prompt tuning\n",
        "    fp16=True,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "B65mwky_f0NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59632076"
      },
      "source": [
        "import subprocess\n",
        "output = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "print(output.stdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./qwen3_prompt_tuning\")"
      ],
      "metadata": {
        "id": "SpZ3HwxNw_ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=256,\n",
        ")\n",
        "\n",
        "prompt = \"<system> You are a helpful assistant. </system>\\n<user>Explain gradient descent simply.</user>\\n<assistant>\"\n",
        "output = pipe(prompt)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "5-wky18W34Ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}