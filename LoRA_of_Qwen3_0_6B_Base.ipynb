{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGEv9_cu1vYs",
    "outputId": "f439337c-7e3c-4764-cce5-b589b5dcfd67"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP3bpnyM1vYw"
   },
   "source": [
    "## Local Inference on GPU\n",
    "Model page: https://huggingface.co/unsloth/Qwen3-0.6B-Base\n",
    "\n",
    "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/unsloth/Qwen3-0.6B-Base)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446,
     "referenced_widgets": [
      "80a7900bb1104f5385fa66f76f259375",
      "c664ee7b0c984fe9add0d9ff83503bb9",
      "8aa68e42767e48f98e5571b2291dce5f",
      "6b94fe658d12406aaa5341d0b785762e",
      "8a25b715b5d546ca8fff8bc64da167b4",
      "caccc5ef5e684798873184fb51d57426",
      "588f6ea7579e45f19bf1330a2971fa36",
      "9ed992b4ae074d1596d2cc87a80a2ac3",
      "7ea4756fde0648a6b582be6c1b259849",
      "be6133902b0f4a97a8589af89a7c5899",
      "11ab585545ab4088b49bb571f498b389",
      "29d5e9ea6ab843868cae77d4ddf098f9",
      "97d99defa8e94bcfaba4ac8681a0e3ee",
      "31bb5dfd9cbe46a5a8dbd3f392264a94",
      "2a24e0b69e14484388bf88edb641e731",
      "d6051607e09340d6bdde03207a400279",
      "42c2935f931b4854ad3817fd186660fa",
      "3dcd9ea79aea4eb0909cf4943249a0ca",
      "0242e22395644a67868f483f13c5baba",
      "14391c0884df4c3689f39233dee2efd1",
      "01a4055004f84facb11810175881c527",
      "d017232a082d4f88bf6a9ab1119d462d",
      "7750db6ddca9497cba39141259d5a610",
      "d36985b0ea0f4441a7d9743ad4835e3c",
      "23ae79e97ec0468790849ba476643e9a",
      "900e128d1fb3444f92004b9e3b732657",
      "31c31eb06f19474c83721e587917b301",
      "66ceee00f96748a1b199fa034ca227a9",
      "979b2b82d8e94ee88f46fd0efda3f26f",
      "15ba0b97659944d79cf9f6038b2c501a",
      "9b45e1d72a7d455aa5301183b9ed8477",
      "c8fbfbaccf9b4ae0ba1d4ed0f5dacea7",
      "49386c492313448c9bc2fe709ca25165",
      "32239878b8294a82b05ca4dc9c64a7f9",
      "b0bb5bbfb1e14d97ba6e674522bbbe45",
      "1e2f6687a0aa417293430fabb0cd3ba4",
      "b2154b3c303d435aa0301ab9d6198ff6",
      "c8cbf604fd634b318073ec51bac7988d",
      "161f338f4d314d31a02aa5909c700d3d",
      "a19996086d1e447885905f5b8ed2bdce",
      "257a82afdfa9464ebdf930e39fb80ecc",
      "9c441606e6c4482c9aba932db04c9ff6",
      "680477f2f48c42d0a71853d6549de677",
      "fac59848186249f7a548c3cb4da48750",
      "9f9b1ca2c981486da14c254e3b5a35f5",
      "7bbf7955417749faa996efd838aac21d",
      "b3952f15a51d4357a0f566ad94e90a86",
      "0c580959b0aa4a23b654293ef272bdf4",
      "161ed8886fa841e292bbac64ce23ae38",
      "1b6317c5a6854e73bbc0fe620169ccf7",
      "593d47f4e1ea4c08b130d06ec2204c18",
      "17e1c78bbbc4479fb11938af7123cb44",
      "30a37be02b9647c0819599c41529b8d6",
      "41249ae8325d495f8f9d77e4cae476ae",
      "ddeffbe067ac41cd86a4dede192ba6f3",
      "3fe496f117a34d2a979a9955e283c62c",
      "a3cd1a504d4945bcadb878f8a6139e41",
      "c441016a7c7241cbae3328f00a0be5f4",
      "12970b51eb55485ab8fb139badfa920e",
      "804c1b1dd84f4e76bf1a59c5dccfdf32",
      "6dbb97ec8b7c42c3b6df5375a50e1760",
      "ab7ea69a102c4d07bf2329964f739433",
      "6541c4e0690640d48444c1f7f78f3269",
      "2d7dcbee633d46eaaeb6a6c08c047744",
      "b2f764e10158420a964dee579ab0454b",
      "1c63b3e284734060bd61d12af2db098a",
      "ab9d14ec96d342bf92cd99ea6a20871e",
      "4e3949dd944048c8a56f9f9035be8122",
      "4ca40f0151b6442d93551f2ba70e901e",
      "7429770b1ac34621b738eaa47d980ad8",
      "38b7de51a7a94f088987e9d829925e18",
      "0d63f6d42a774a078f7936aa90dd35d9",
      "0d7ea9dd764444a7b934033125ceb40e",
      "925bc4f7409f4216850c095d78b8525b",
      "bebadf407106489aa07691b49f46cf06",
      "e2bb2853b5cc4f22b6e3f1f556c9be69",
      "061ad65447ba433c8e5fe81865b29fdc",
      "46f19d1dff5346fe8314b56ee94ba52d",
      "fbbb5c1a78d24a3691a52d9356cc919e",
      "1ad143eb804b4eb69cc14f41190dad52",
      "6a01ed2462614d538411623867839c3e",
      "b1f2123b5dfd415c86eb016b7d2f81c8",
      "2b0c21f39de04a5eb701fc93e2c07aca",
      "a681527c8c1e4cf7ae30079edc2c76bf",
      "bac78b450c38478a92909024c4815835",
      "15cb2499e3a347249be5764ef1961d48",
      "0accdaf707124c7aa9bf355eaeb5da93",
      "92fbfe6e76d44d6ea44a7fb0dd7c4a9b",
      "e0a659526c63408e9133606b3c2c5a15",
      "8f6c06bd19c841c1a83b553086c1a50e",
      "77ffeb3bd2704aceb6272aef264ff222",
      "d8f3a4526db54c359ba50b0f2d28991f",
      "3221fa3fd5934188928c1e9659a90b03",
      "4350fbcad6564411ae11cf8191d890fc",
      "472b5c1900244db9a4f4699e76f81349",
      "a30d6e0e844f45fe83d24cb6515f7ef6",
      "9f810f754c8c4cc9a84fbc8c995d8946",
      "0390ad54f7804fed814a37467c136773",
      "75596b73f3794be79a222429c44238ae"
     ]
    },
    "id": "M7lCqhVn1vYz",
    "outputId": "d9b280bf-ae57-4d6d-eb29-55d593b51327"
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"unsloth/Qwen3-0.6B-Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcBR-Vam1vY1"
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-0.6B-Base\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen3-0.6B-Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgiykS6o14Sz",
    "outputId": "d15fb325-9701-44ea-ff85-da03ad8dcdec"
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-0.6B-Base\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen3-0.6B-Base\", dtype=torch.float16, device_map = \"auto\")\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeOWYMPZ2DfF",
    "outputId": "3943ef3c-49c9-4eed-d02f-efbff25eea50"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"vicgalle/alpaca-gpt4\")\n",
    "\n",
    "ds = ds[\"train\"]\n",
    "print(ds)\n",
    "dataset = ds.shuffle(seed = 42)\n",
    "print(dataset)\n",
    "\n",
    "train_dataset = dataset.select(range(2000))\n",
    "test_dataset = dataset.select(range(2000,2100))\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "24ebc69324d34927a435eddc4d7f31f5",
      "c4b103709a754f9f870c4eb64af9f3d4",
      "585fde371bd34c3bb5b525fd88725831",
      "da8c25a5b27042edab9952c5292de08d",
      "57d782cd646145e5a4cd97252382cdf8",
      "4fe26a170f5141e08c2e7d3be76fc751",
      "e9557f435ef64c65b5bc088a86a47438",
      "e82be6710ae0466a8c742b862ef4ee85",
      "4acb806a820648a88799e23a0537c0dd",
      "fb8a10a05dcd4b11b13c74f75f939994",
      "c0c51998b9f640459b7e1f00f2ecdde7",
      "0bc2ebee17a14d28a30816f6f19b7976",
      "84279f5689754137989a36e2cddaf899",
      "bbe1cb19ddcf4b3983ee641b49f7fdd8",
      "5ef1be71589349858f412cdbc4ddfaf5",
      "110637e7b6d84e1193ddd19d59613f97",
      "44301114d9ce4e42803bc983ba9d593b",
      "096ecdc9487a4f19855fd76d4b1f4e09",
      "a543ffd7ff2d4fe7bd0ae5bd6bd20bed",
      "ac73c01febd54c38b532484c6c8d4563",
      "0bb5f931f4ca498eb1c0959e863496ea",
      "c64bd83cf0e54be8b8309a2f23f0140d"
     ]
    },
    "id": "0jzwRKPm2HPK",
    "outputId": "8bcef157-1188-447a-cd60-fda9634077eb"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "def preprocess(example):\n",
    "    # Combine instruction + optional input + output\n",
    "    prompt = f\"<system> You are a helpful assistant. </system>\\n\" \\\n",
    "             f\"<user>{example['instruction']}\\n{example.get('input','')}</user>\\n\" \\\n",
    "             f\"<assistant>{example['output']}</assistant>\"\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Labels for causal LM\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess, batched=False)\n",
    "test_dataset = test_dataset.map(preprocess, batched=False)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3TTavGLaGb7",
    "outputId": "c571d36f-83be-42b2-9912-df525ccda80b"
   },
   "outputs": [],
   "source": [
    "# Number of tokens in train dataset\n",
    "total_train_tokens = sum(len(x[\"input_ids\"]) for x in train_dataset)\n",
    "\n",
    "# Number of tokens in test dataset\n",
    "total_test_tokens = sum(len(x[\"input_ids\"]) for x in test_dataset)\n",
    "\n",
    "print(f\"Total train tokens: {total_train_tokens}\")\n",
    "print(f\"Total test tokens: {total_test_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Acvcv4TkcR1o",
    "outputId": "bbc33dab-e6f6-4f36-fe7e-69c56c98eb75"
   },
   "outputs": [],
   "source": [
    "avg_train_tokens = total_train_tokens / len(train_dataset)\n",
    "avg_test_tokens  = total_test_tokens / len(test_dataset)\n",
    "\n",
    "print(f\"Average train tokens per example: {avg_train_tokens:.1f}\")\n",
    "print(f\"Average test tokens per example: {avg_test_tokens:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsDg6eld2KyK",
    "outputId": "b243a3a7-415c-4bde-a4c7-061846c71357"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "c4dqkpVL2nOC",
    "outputId": "74a99c5d-28af-4464-9c79-c2d0025a14fa"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen3_lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # effective batch = 16\n",
    "    learning_rate=1e-4,             # higher LR works well for prompt tuning\n",
    "    fp16=True,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VJP1_Pn3w_T"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./qwen3_LoRA_tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kT4xHDGW3Z8T",
    "outputId": "29b6d63f-edc2-471d-f890-6811dfa50ea1"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "prompt = \"<system> You are a helpful assistant. </system>\\n<user>Explain gradient descent simply.</user>\\n<assistant>\"\n",
    "output = pipe(prompt)\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
